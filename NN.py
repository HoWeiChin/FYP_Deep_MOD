import sys
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os

path_to_deep_mod = '/gpfs0/home/e0031794/Documents/DeePyMoD/src'
sys.path.append(path_to_deep_mod)

from deepymod.DeepMoD import DeepMoD
from deepymod.library_functions import library_1D
from deepymod.utilities import library_matrix_mat, print_PDE, tensorboard_to_dataframe

from sklearn.model_selection import train_test_split

#import from other .py file
from DimReduction import reduce_dim

class GPUManager:
    """
        Manages GPU allocation during deep learning tasks
    """
    def get_free_gpu(self):

        """
            Selects the gpu with the most free memory

        """
        import subprocess
        import numpy as np

        output = subprocess.Popen('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free', stdout=subprocess.PIPE,
                              shell=True).communicate()[0]
        output = output.decode("ascii")
        # assumes that it is on the popiah server and the last gpu is not used
        memory_available = [int(x.split()[2]) for x in output.split("\n")[:-2]]
        if not memory_available:
            return
        print("Setting GPU to use to PID {}".format(np.argmax(memory_available)))
        return np.argmax(memory_available)
    
    def set_one_gpu(self):

        gpu = str(self.get_free_gpu())

        if not gpu:
            return

        print("Using GPU: %s" % gpu)
        os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  # see issue #152
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu

class PDELibraryManager:
    """
        Configures basis PDE terms for DeepMOD training
    """

    def __init__(self):
        self.u = ['1', 'u', 'uË†2']
        self.du = ['1', 'u_{x}', 'u_{xx}', 'u_{xxx}']
        self.coeffs_list = library_matrix_mat(self.u, self.du)
        self.library_config = {'total_terms': len(self.coeffs_list), 'deriv_order': 3, 'poly_order': 2}
    
    def get_library_config(self):
        return self.library_config
    
    def get_coeffs_list(self):
        return self.coeffs_list

class DeepMODConfigurator:
    """
        Configures architectural layer of DeepMOD and its training hypyerparameters and output configurations
    """
    def __init__(self, nn_layer_config, training_hyper_param, output_config):

        self.nn_layer_config = nn_layer_config
        self.training_hyper_param = training_hyper_param
        self.output_config = output_config
    
    def get_nn_layer_config(self):
        return self.nn_layer_config

    def get_training_hyper_param(self):
        return self.training_hyper_param
    
    def get_output_config(self):
        return self.output_config

class TrainingManager:
    """
        Manages Training of DeepMod

    """
    def __init__(self, nn_layer_config, training_hyper_param, output_config):
        self.GPUManager = GPUManager()
        self.PDELibManager = PDELibraryManager()
        self.DeepMODConfigurator = DeepMODConfigurator(nn_layer_config, training_hyper_param, output_config)
    
    def train_model(self, X_train, Y_train):

        """
            X_train (np array): training data in numpy format
            Y_train (np array): ground truth (your Y labels) in numpy format
        
        """
        self.GPUManager.set_one_gpu()
        lib_config = self.PDELibManager.get_library_config()
        config = self.DeepMODConfigurator.get_nn_layer_config()
        training_hyper_param = self.DeepMODConfigurator.get_training_hyper_param()
        output_config = self.DeepMODConfigurator.get_output_config() #for saving DeepMod's model parameter
        print(output_config)
        sparse_vectors, denoised = DeepMoD(
            X_train, Y_train, config, library_1D, 
            lib_config, training_hyper_param, 
            output_config)
        
        return sparse_vectors, denoised

class ResultProcessor:

    """
        Provides functionalities to print discovered PDEs or plot comparison of reconstructed vs original data

    """
    def __init__(self):
        self.PDElibManager = PDELibraryManager()
        

    def plot_comparison(self, original_data, ground_truth, inferred_data, image_name):
        
        """
            original_data (np array): original training data generated by the pde
            ground_truth (np array): original bicoid concentration generated by the pde
            inferred_data (np array): bicoid concentration generated by DeepMoD
            image_name (string): name of image with .png or .jpg extension
            function will save plots

        """
        data_dict = {'x_grid': original_data[:, 0], 't_grid': original_data[:, 1], 
                     'ground truth': ground_truth, 'inferred': inferred_data}
        for key in data_dict:
            data_dict[key] = np.squeeze(data_dict[key])  # pandas doesn't like higher dimensional arrays
        df = pd.DataFrame(data_dict)
        
        fig, axes = plt.subplots(ncols=2, figsize=(20, 4))
        
        #original image   
        im_o = axes[0].contourf(df.pivot(index='t_grid', columns='x_grid', values='ground truth'), cmap='coolwarm')
        axes[0].set_xlabel('x')
        axes[0].set_ylabel('t')
        axes[0].set_title('Ground truth')
        
        #reconstructed image
        im_recon = axes[1].contourf(df.pivot(index='t_grid', columns='x_grid', values='inferred'), cmap='coolwarm')
        axes[1].set_xlabel('x')
        axes[1].set_title('Reconstructed')
        fig.colorbar(im_recon, ax=axes.ravel().tolist())
        plt.show()
        plt.savefig(image_name)
    
    def print_discovered_model(self, sparse_vector, save_file_name):
        coeffs_list = self.PDElibManager.get_coeffs_list()
        result = print_PDE(sparse_vector[0], coeffs_list, PDE_term="u_t")
        with open(save_file_name, 'w+') as f:
            f.write(result)

class Validator:

    # a wrapper to load and perform a train-test (or validation) split
    def load_and_split_data(self, path_to_x, path_to_y, val_size):

        """
            path_to_x (str): full path to the X data matrix
            path_to_y (str): full path to the Y label vector/matrix
            val_size (float): size of validation set (i.e 0.2 = 20% of data belongs to a validation set)

            return:
            X np array (train), Y np array (train), X np array (validate), Y np array (validate) 

        """
        np_X = np.load(path_to_x)
        np_Y = np.load(path_to_y)
        X_train, X_val, Y_train, Y_val = train_test_split(np_X, np_Y, test_size=val_size)

        return X_train, X_val, Y_train, Y_val

def pipeline(data_dir):
    #data_dir is an absolute path
    all_files_folders = os.listdir(data_dir)
    target_folders = [os.path.join(data_dir, obj) for obj in all_files_folders if '_subset' in obj] #search for subfolders

    nn_layer_config = {'layers': [2, 20, 20, 20, 20, 20, 1], 'lambda': 10e-6}
    training_config = {'max_iterations': 50000, 'grad_tol': 10**-6, 'learning_rate': 0.002, 'beta1': 0.99, 
        'beta2': 0.999, 'epsilon': 10**-8}
    output_config ={"output_directory": '', 'X_predict': ''}
    result_manager = ResultProcessor()

    #subset == amt of subset from original data. i.e amt of slicing done
    for folder in target_folders:
        subset = folder.split('/') [-1].split('_')[0]
        os.chdir(folder)
        curr_dir = os.getcwd()
        full_x_t = np.load(os.path.join(curr_dir, 'space_time_data_full_' + subset + '.npy'))
        x_t_train = np.load(os.path.join(curr_dir, 'space_time_data_sampled_' + subset + '.npy'))
        bicoid_train = np.load(os.path.join(curr_dir, 'bicoid_sampled_' + subset + '.npy'))
        output_config['output_directory'] = os.path.join(curr_dir, 'Out_subset_' + subset + '_expt')
        output_config['X_predict'] = full_x_t

        t_manager = TrainingManager(nn_layer_config, training_config, output_config)
        sparse_vector, denoised = t_manager.train_model(x_t_train, bicoid_train)

        #print discovered pde:
        save_file_path = os.path.join(data_dir, 'pde_result_subset_' + subset + '.txt')
        result_manager.print_discovered_model(sparse_vector, save_file_path)

        sparse_path = os.path.join(data_dir, 'sparse_vec_subset' + subset)
        denoised_path = os.path.join(data_dir, 'denoised_result_subset' + subset)

        #save training output
        np.save(sparse_path, sparse_vector)
        np.save(denoised_path, denoised)

if __name__ == "__main__":
    data_dir = '/gpfs0/home/e0031794/Documents/FYP/FYP_results_11_9_2019/data_slicing/1_trial'
    pipeline(data_dir)

















    


    